# 1. 컴퓨터 내부의 언어 체계

## 언어란 무엇인가
**컴퓨터 언어**를 포함한 모든 언어의 뜻은 기호의 집합으로 인코딩(encoding)된다.<br>
이를 설명하기 위해 언어를 세 가지 구성요소로 나눌 수 있다:
- 기호가 들어갈 상자
- 상자에 들어갈 기호
- 상자의 순서


## 비트
비유된 상자는 자연어에서 **문자(character)**, 컴퓨터 언어에서는 **비트(bit)** 를 뜻한다.  
> '비트'는 2진법을 뜻하는 '바이너리(binary)'와 숫자를 의미하는 'digit'의 결합어

컴퓨터 언어에서 비트는 [1/0], [켜짐/꺼짐], [참/거짓] 같은 2진법의 기호를 담는 상자 역할을 한다.

## 논리 연산
논리 연산은 하나 이상의 입력값을 논리적으로 결합해 결과를 도출하는 연산이다. <br>
예를 들어, "배가 고픈가"가 참이고 "다이어트 중인가"가 거짓일 때 "밥을 먹는다"가 참이 되는 조건문이 논리 연산의 예다.

### 불리언 대수
조지 불(George Boole)은 비트 연산 규칙의 집합인 **불리언 대수(Boolean algebra)**를 정의했다.<br>
기본 연산자는 **NOT, AND, OR**이고, 추가로 **XOR**도 자주 사용된다. 또한 일반 대수와 마찬가지로 결합 법칙, 교환 법칙, 분배 법칙을 불리언 대수에 적용할 수 있다.

- **NOT**: 입력값을 뒤집는다.
- **AND**: 모든 입력값이 참일 때만 결과가 참이다.
- **OR**: 하나라도 참이면 결과가 참이다.
- **XOR**: 입력값이 다를 때 참이다. 세 비트 이상에서도 참의 개수가 홀수면 참이다.


<img width="697" alt="image" src="https://github.com/user-attachments/assets/23a1972e-5799-4ec5-95d4-f6622ec4f978" />

> 불리언 연산 진리표(truth table). TRUE는 1로, FALSE는 0으로 대응된다

### 드모르간의 법칙
오거스터스 드모르간(Augustus De Morgan)은 불리언 대수에 적용할 수 있는 **드모르간의 법칙** (De Morgan's law)을 새로 발견했다.<br>
이 법칙은 다음 두 가지 식으로 표현된다:
1. NOT (A AND B) = NOT A OR NOT B
2. NOT (A OR B) = NOT A AND NOT B
<img width="765" alt="image" src="https://github.com/user-attachments/assets/8dd4b8f4-4fdb-4db2-9af6-5d3f4a0bb90e" />

<img width="279" alt="image" src="https://github.com/user-attachments/assets/1e822b68-a826-465c-b3d0-6c22dbb4c3a3" />


드모르간의 법칙을 통해, NOT 연산을 사용할 경우 AND연산과 OR연산이 서로 대체 가능함을 알 수 있다.

### 정수를 비트로 표현하는 방법

#### 양의 정수 표현
10진수(decimal number) 체계에서는 값을 10의 거듭제곱으로 표현한다. 이를 밑(base)이 10이라고 한다.  
<img width="365" alt="image" src="https://github.com/user-attachments/assets/1ced2c3c-bf96-4f2c-acf4-42f926e69bcb" />

2진수(binary number)는 0과 1 두 가지 숫자만 사용해서 숫자를 표현하는 방법이다.<br>
각 자릿수는 오른쪽에서 왼쪽으로 이동하며 2의 제곱을 의미한다.<br>
각 자릿수의 값을 더하면 10진수(decimal) 숫자와 동일한 값을 얻을 수 있다.

<img width="498" alt="image" src="https://github.com/user-attachments/assets/5e3c2deb-293d-464e-8016-b97c7b575fae" />


$$
1001110100100_{(2)} = 1 \cdot 2^{12} + 0 \cdot 2^{11} + 0 \cdot 2^{10} + 1 \cdot 2^9 + 1 \cdot 2^8 + 1 \cdot 2^7 + 0 \cdot 2^6 + 0 \cdot 2^5 + 1 \cdot 2^4 + 0 \cdot 2^3 + 0 \cdot 2^2 + 1 \cdot 2^1 + 0 \cdot 2^0
$$

$$
= 4096 + 0 + 0 + 512 + 256 + 128 + 0 + 0 + 16 + 0 + 0 + 2 + 0
$$

$$
= 5028_{(10)}
$$

+ 2진수에서 가장 오른쪽의 비트를 가장 작은 유효 비트(Least significant bit; **LSB**)라고 하고,<br> 가장 왼쪽의 비트를 가장 큰 유효 비트(Most significant bit; **MSB**)라고 한다.

#### 2진수 덧셈

10진 덧셈과 동일하게 2진수에서도 각 비트를 LSB에서 MSB 쪽으로 더하며 결과가 1보다 크면 1을 다음 자리로 옮긴다.<br>

ex) 1은 2진수로 001이고, 5는 2진수로 101이다. 이를 합하면 110(2) = 6(10)이 된다.

<img width="426" alt="스크린샷 2024-12-26 오후 9 47 27" src="https://github.com/user-attachments/assets/26992324-5e26-4a22-b292-830aa25c6f0a" />

> 컴퓨터 하드웨어가 2진수 덧셈을 수행할 때는 AND 연산과 XOR 연산을 사용한다.

그러나 덧셈 결과가 사용할 비트의 개수로 표현될 수 있는 범위를 벗어나면 어떻게 될까?  
이 경우 MSB에서 올림이 발생하게 되며, 이를 **오버플로(overflow)** 라고 한다.

#### 예시: 4비트 덧셈에서 오버플로
4비트 연산에서 \(1001_2\) + \(1000_2\)를 계산한다고 가정해보자.  
- \(1001_2 = 9_{10}\), \(1000_2 = 8_{10}\)  
- 이 둘을 더하면 실제 결과는 \(10001_2 = 17_{10}\)이 되어야 한다.  

하지만, **4비트의 범위(0~15, 즉 \(0000_2\) ~ \(1111_2\))**를 벗어나기 때문에 MSB 왼쪽에 추가된 비트를 저장할 공간이 없어, 결과가 **0001**이 된다.  
즉, \(17_{10}\)이 아닌 \(1_{10}\)로 계산된다.

#### 오버플로 감지
컴퓨터는 이러한 오버플로를 감지하기 위해 **조건 코드 레지스터(Condition Code Register)** 를 사용한다.  
이 레지스터는 연산 중 발생한 특정 조건(예: 오버플로, 캐리 발생, 부호 변화 등)을 나타내는 **오버플로 비트** 를 포함한다. 이를 통해 프로그램은 오버플로를 감지하고 적절한 처리를 할 수 있다.

#### 음수 표현
2진수에선 뺄셈을 표현하기 위해 새로운 회로를 규정하지 않는다. 대신, 양수와 음수를 나타내는 비트를 MSB에 놓아 부호(sign)처럼 사용하기로 했다.(0은 양수, 1은 음수)<br>

음수를 표현 하기 위해 세가지 방법이 있는데 순서대로 알아보자.

<img width="962" alt="image" src="https://github.com/user-attachments/assets/84561dd3-30cc-4b7c-b74a-9ac827ebabf7" />

1. Signed-magnitude(부호화 크기 표현법)
   - 음수를 표현하기 위해 MSB를 반전시켜 부호를 나타내고, 나머지 비트는 값의 크기를 그대로 유지.
   - 단점 : 0 중복 표현 문제, XOR과 AND를 통한 덧셈 계산 불가
2. One's complement(1의 보수)
   - 음수를 표현하기 위해 모든 비트를 반전 시킴. 덧셈 시 MSB에서 발생한 올림(carry)을 LSB로 전달하는 순환올림(end-around carry)을 추가로 수행.
   - 단점 : 0 중복 표현 문제, 덧셈 과정이 복잡(순환 올림 계산 추가 수행)
3. Two's complement(2의 보수)
   - 음수를 표현하기 위해 모든 비트를 반전 시킨 후 LSB에 1을 더함. MSB에 캐리 발생시 이 값은 버림
   - 0 중복 표현 문제가 없고, 덧셈이 자연스럽게 됨
   - 현대 컴퓨터의 표준 방식
